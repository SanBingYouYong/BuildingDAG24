{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define the encoder network using pre-trained VGG16\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(vgg16.features.children()))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.fc = nn.Linear(512 * 7 * 7, 4096)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define the decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the encoder and decoder networks\n",
    "encoder = Encoder()\n",
    "decoders = []\n",
    "params = ['param1', 'param2', 'param3']  # Replace with actual parameter names\n",
    "\n",
    "for param in params:\n",
    "    decoder = Decoder(4096, output_size)  # Replace output_size with the actual size of the parameter\n",
    "    decoders.append(decoder)\n",
    "\n",
    "# Load the input image\n",
    "image_path = './datasets/test_dataset/images/image.jpg'  # Replace with the actual image path\n",
    "image = torch.randn(1, 1, 224, 224)  # Replace with the actual image tensor\n",
    "\n",
    "# Encode the image\n",
    "embedding = encoder(image)\n",
    "\n",
    "# Decode the parameters\n",
    "decoded_params = []\n",
    "for decoder in decoders:\n",
    "    decoded_param = decoder(embedding)\n",
    "    decoded_params.append(decoded_param)\n",
    "\n",
    "# Print the decoded parameters\n",
    "for param, decoded_param in zip(params, decoded_params):\n",
    "    print(f'{param}: {decoded_param}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import yaml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define your custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, params_folder, normalization_ranges, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.params_folder = params_folder\n",
    "        self.normalization_ranges = normalization_ranges\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_filenames = [filename for filename in os.listdir(image_folder) if filename.endswith(\".png\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.image_filenames[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_folder, img_filename)\n",
    "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Load corresponding parameters\n",
    "        param_filename = os.path.splitext(img_filename)[0] + \".yml\"\n",
    "        param_path = os.path.join(self.params_folder, param_filename)\n",
    "        with open(param_path, \"r\") as param_file:\n",
    "            param_data = yaml.load(param_file, Loader=yaml.FullLoader)\n",
    "\n",
    "        # Normalize parameters based on the provided ranges\n",
    "        for key, value in param_data.items():\n",
    "            if key in self.normalization_ranges:\n",
    "                if self.normalization_ranges[key]['type'] == 'float':\n",
    "                    param_data[key] = (value - self.normalization_ranges[key]['min']) / (self.normalization_ranges[key]['max'] - self.normalization_ranges[key]['min'])\n",
    "                elif self.normalization_ranges[key]['type'] == 'int':\n",
    "                    param_data[key] = (value - self.normalization_ranges[key]['min']) / (self.normalization_ranges[key]['max'] - self.normalization_ranges[key]['min'])\n",
    "                elif self.normalization_ranges[key]['type'] == 'vector':\n",
    "                    for dim in ['x', 'y', 'z']:\n",
    "                        param_data[key][dim] = (value[dim] - self.normalization_ranges[key][f'{dim}min']) / (self.normalization_ranges[key][f'{dim}max'] - self.normalization_ranges[key][f'{dim}min'])\n",
    "                elif self.normalization_ranges[key]['type'] == 'states':\n",
    "                    # Convert states to one-hot encoding\n",
    "                    states = self.normalization_ranges[key]['values']\n",
    "                    param_data[key] = [1 if state == value else 0 for state in states]\n",
    "                elif self.normalization_ranges[key]['type'] == 'bool':\n",
    "                    # Convert bool to 0 or 1\n",
    "                    param_data[key] = 1 if value else 0\n",
    "\n",
    "        return img, param_data\n",
    "\n",
    "\n",
    "# Define the custom decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load dataset and normalization ranges\n",
    "dataset_path = \"./datasets/test_dataset\"\n",
    "image_folder = os.path.join(dataset_path, \"images\")\n",
    "params_folder = os.path.join(dataset_path, \"params\")\n",
    "ranges_file = os.path.join(dataset_path, \"ranges.yml\")\n",
    "\n",
    "with open(ranges_file, \"r\") as file:\n",
    "    normalization_ranges = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Create dataset instance\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "custom_dataset = CustomDataset(image_folder, params_folder, normalization_ranges, transform)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define VGG model as encoder\n",
    "vgg_model = models.vgg16(pretrained=True)\n",
    "vgg_model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "encoder = vgg_model.features\n",
    "\n",
    "# Create decoder instance\n",
    "decoder_input_size = 512  # Adjust based on the output of the encoder\n",
    "decoder_output_size = your_output_size  # Adjust based on your parameter output size\n",
    "decoder = Decoder(decoder_input_size, decoder_output_size)\n",
    "\n",
    "# Define your model\n",
    "class YourModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output of the encoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate your model\n",
    "model = YourModel(encoder, decoder)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        images, params = batch\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, params)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save your trained model if needed\n",
    "torch.save(model.state_dict(), \"your_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Example VGG-like architecture for demonstration\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256 * 28 * 28, 4096)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "model = Encoder()\n",
    "# check output size\n",
    "x = torch.randn(1, 1, 224, 224)\n",
    "output = model(x)\n",
    "print(output.size())  # torch.Size([1, 256, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder 1 output shape: torch.Size([2, 10])\n",
      "tensor([[ 0.0023, -0.0169,  0.0799,  0.0611, -0.0463, -0.0411,  0.0378, -0.0634,\n",
      "          0.0316, -0.0387],\n",
      "        [ 0.0023, -0.0129,  0.0778,  0.0571, -0.0494, -0.0460,  0.0412, -0.0643,\n",
      "          0.0299, -0.0415]], grad_fn=<AddmmBackward0>)\n",
      "Decoder 2 output shape: torch.Size([2, 10])\n",
      "tensor([[ 0.0568,  0.0541,  0.0270, -0.0402,  0.0625, -0.0515,  0.0577, -0.0658,\n",
      "          0.0035, -0.0482],\n",
      "        [ 0.0597,  0.0535,  0.0264, -0.0321,  0.0657, -0.0503,  0.0589, -0.0611,\n",
      "          0.0045, -0.0452]], grad_fn=<AddmmBackward0>)\n",
      "Decoder 3 output shape: torch.Size([2, 10])\n",
      "tensor([[ 0.0510,  0.0615, -0.0405,  0.0265,  0.0909, -0.0070, -0.0178, -0.0118,\n",
      "         -0.0439,  0.0020],\n",
      "        [ 0.0467,  0.0568, -0.0375,  0.0309,  0.0885, -0.0070, -0.0252, -0.0161,\n",
      "         -0.0382,  0.0054]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Encoder-Decoders (own)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Encoder using VGG (you may replace it with your desired encoder)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Example VGG-like architecture for demonstration\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 28 * 28, 4096)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Define Decoder using a simple 3-layer MLP\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Complete model with one Encoder and multiple Decoders\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder_count, decoder_output_size):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = nn.ModuleList([Decoder(4096, decoder_output_size) for _ in range(decoder_count)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        decoder_outputs = [decoder(x) for decoder in self.decoders]\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Create an instance of the EncoderDecoderModel\n",
    "    encoder = Encoder()\n",
    "    decoder_count = 3\n",
    "    decoder_output_size = 10  # Change this to your desired output size\n",
    "    model = EncoderDecoderModel(encoder, decoder_count, decoder_output_size)\n",
    "\n",
    "    # Example input tensor with shape (batch_size, channels, height, width)\n",
    "    input_tensor = torch.randn(2, 1, 224, 224)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Print the output shapes for each decoder\n",
    "    for i, decoder_output in enumerate(output):\n",
    "        print(f\"Decoder {i + 1} output shape: {decoder_output.shape}\")\n",
    "        print(decoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsy/miniconda3/envs/dag/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zsy/miniconda3/envs/dag/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/zsy/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:20<00:00, 26.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder 1 output shape: torch.Size([2, 10])\n",
      "Decoder 2 output shape: torch.Size([2, 10])\n",
      "Decoder 3 output shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Encoder-decoders (VGG16 Encoder)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define Encoder using a pre-trained VGG16\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        # Remove the fully connected layers to get features only\n",
    "        self.features = nn.Sequential(*list(vgg16.features.children()))\n",
    "\n",
    "        # Set parameters of the encoder to be non-trainable\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Define Decoder using a simple 3-layer MLP\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Complete model with one Pre-trained Encoder and multiple Decoders\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder_count, decoder_output_size):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = nn.ModuleList([Decoder(512 * 16 * 16, decoder_output_size) for _ in range(decoder_count)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        decoder_outputs = [decoder(x.view(x.size(0), -1)) for decoder in self.decoders]\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Create an instance of the EncoderDecoderModel with a pre-trained encoder\n",
    "    encoder = Encoder()\n",
    "    decoder_count = 3\n",
    "    decoder_output_size = 10  # Change this to your desired output size\n",
    "    model = EncoderDecoderModel(encoder, decoder_count, decoder_output_size)\n",
    "\n",
    "    # Example input tensor with shape (batch_size, channels, height, width)\n",
    "    input_tensor = torch.randn(2, 3, 512, 512)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Print the output shapes for each decoder\n",
    "    for i, decoder_output in enumerate(output):\n",
    "        print(f\"Decoder {i + 1} output shape: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 16, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x131072 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Print the output shapes for each decoder\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, decoder_output \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 78\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 78\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m {param_name: decoder(x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     79\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m param_name, decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs\n",
      "Cell \u001b[0;32mIn[29], line 78\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 78\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m {param_name: \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m param_name, decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 33\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x131072 and 512x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define Encoder using a pre-trained VGG16\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        # Remove the fully connected layers to get features only\n",
    "        self.features = nn.Sequential(*list(vgg16.features.children()))\n",
    "\n",
    "        # Set parameters of the encoder to be non-trainable\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the ranges from the YAML file\n",
    "import yaml\n",
    "\n",
    "with open('./datasets/test_dataset/ranges.yml', 'r') as file:\n",
    "    ranges = yaml.safe_load(file)\n",
    "\n",
    "# Create a mapping between parameter names and output sizes\n",
    "parameter_output_mapping = {}\n",
    "for param_name, param_specs in ranges.items():\n",
    "    if param_specs['type'] == 'float':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'int':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'vector':\n",
    "        parameter_output_mapping[param_name] = 3  # 3 for x, y, z\n",
    "    elif param_specs['type'] == 'states':\n",
    "        parameter_output_mapping[param_name] = len(param_specs['values'])\n",
    "    elif param_specs['type'] == 'bool':\n",
    "        parameter_output_mapping[param_name] = 2  # 2 for binary encoding\n",
    "\n",
    "\n",
    "\n",
    "# Create decoders based on the mapping\n",
    "decoders = nn.ModuleDict({\n",
    "    param_name: Decoder(512, output_size)\n",
    "    for param_name, output_size in parameter_output_mapping.items()\n",
    "})\n",
    "\n",
    "# Complete model with one Pre-trained Encoder and multiple Decoders\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoders):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = decoders\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        print(x.size())\n",
    "        decoder_outputs = {param_name: decoder(x.view(x.size(0), -1))\n",
    "                           for param_name, decoder in self.decoders.items()}\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Create an instance of the EncoderDecoderModel with a pre-trained encoder\n",
    "encoder = Encoder()  # Use your pre-trained encoder here\n",
    "model = EncoderDecoderModel(encoder, decoders)\n",
    "\n",
    "# Example input tensor with shape (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(2, 3, 512, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output shapes for each decoder\n",
    "for param_name, decoder_output in output.items():\n",
    "    print(f\"{param_name} decoder output shape: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x25088 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Print the output shapes for each decoder\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, decoder_output \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 75\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Get the batch size\u001b[39;00m\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the feature tensor, considering the batch size\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m {param_name: decoder(x) \u001b[38;5;28;01mfor\u001b[39;00m param_name, decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs\n",
      "Cell \u001b[0;32mIn[34], line 75\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Get the batch size\u001b[39;00m\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the feature tensor, considering the batch size\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m {param_name: \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m param_name, decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 33\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dag/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x25088 and 512x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import yaml\n",
    "\n",
    "# Define Encoder using a pre-trained VGG16\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        # Remove the fully connected layers to get features only\n",
    "        self.features = nn.Sequential(*list(vgg16.features.children()))\n",
    "\n",
    "        # Set parameters of the encoder to be non-trainable\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the ranges from the YAML file\n",
    "with open('./datasets/test_dataset/ranges.yml', 'r') as file:\n",
    "    ranges = yaml.safe_load(file)\n",
    "\n",
    "# Create a mapping between parameter names and output sizes\n",
    "parameter_output_mapping = {}\n",
    "for param_name, param_specs in ranges.items():\n",
    "    if param_specs['type'] == 'float':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'int':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'vector':\n",
    "        parameter_output_mapping[param_name] = 3  # 3 for x, y, z\n",
    "    elif param_specs['type'] == 'states':\n",
    "        parameter_output_mapping[param_name] = len(param_specs['values'])\n",
    "    elif param_specs['type'] == 'bool':\n",
    "        parameter_output_mapping[param_name] = 2  # 2 for binary encoding\n",
    "\n",
    "# Create decoders based on the mapping\n",
    "decoders = nn.ModuleDict({\n",
    "    param_name: Decoder(512, output_size)\n",
    "    for param_name, output_size in parameter_output_mapping.items()\n",
    "})\n",
    "\n",
    "# Complete model with one Pre-trained Encoder and multiple Decoders\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoders):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = decoders\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        batch_size = x.size(0)  # Get the batch size\n",
    "        x = x.view(batch_size, -1)  # Flatten the feature tensor, considering the batch size\n",
    "        decoder_outputs = {param_name: decoder(x) for param_name, decoder in self.decoders.items()}\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Create an instance of the EncoderDecoderModel with a pre-trained encoder\n",
    "encoder = Encoder()  # Use your pre-trained encoder here\n",
    "model = EncoderDecoderModel(encoder, decoders)\n",
    "\n",
    "# Example input tensor with shape (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output shapes for each decoder\n",
    "for param_name, decoder_output in output.items():\n",
    "    print(f\"{param_name} decoder output shape: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoders (own)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Encoder using VGG (you may replace it with your desired encoder)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Example VGG-like architecture for demonstration\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 28 * 28, 4096)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Define Decoder using a simple 3-layer MLP\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Complete model with one Encoder and multiple Decoders\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder_count, decoder_output_size):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = nn.ModuleList([Decoder(4096, decoder_output_size) for _ in range(decoder_count)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        decoder_outputs = [decoder(x) for decoder in self.decoders]\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Create an instance of the EncoderDecoderModel\n",
    "    encoder = Encoder()\n",
    "    decoder_count = 3\n",
    "    decoder_output_size = 10  # Change this to your desired output size\n",
    "    model = EncoderDecoderModel(encoder, decoder_count, decoder_output_size)\n",
    "\n",
    "    # Example input tensor with shape (batch_size, channels, height, width)\n",
    "    input_tensor = torch.randn(2, 1, 224, 224)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Print the output shapes for each decoder\n",
    "    for i, decoder_output in enumerate(output):\n",
    "        print(f\"Decoder {i + 1} output shape: {decoder_output.shape}\")\n",
    "        print(decoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoders (own)\n",
    "\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Encoder using VGG (you may replace it with your desired encoder)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Example VGG-like architecture for demonstration\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # size: 512x512x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # size: 256x256x16\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # size: 256x256x32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # size: 128x128x32\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # size: 128x128x64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # size: 64x64x64\n",
    "            nn.Flatten(),  # size: 64x64x64\n",
    "            nn.Linear(64 * 64 * 64, 4096)  # size: 4096\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Define Decoder using a simple 3-layer MLP\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the ranges from the YAML file\n",
    "with open('./ranges.yml', 'r') as file:\n",
    "    ranges = yaml.safe_load(file)\n",
    "\n",
    "# Create a mapping between parameter names and output sizes\n",
    "parameter_output_mapping = {}\n",
    "for param_name, param_specs in ranges.items():\n",
    "    if param_specs['type'] == 'float':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'int':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'vector':\n",
    "        parameter_output_mapping[param_name] = 3  # 3 for x, y, z\n",
    "    elif param_specs['type'] == 'states':\n",
    "        parameter_output_mapping[param_name] = len(param_specs['values'])\n",
    "    elif param_specs['type'] == 'bool':\n",
    "        parameter_output_mapping[param_name] = 2  # 2 for binary encoding\n",
    "\n",
    "# Create decoders based on the mapping\n",
    "decoders = nn.ModuleDict({\n",
    "    param_name: Decoder(512, output_size)\n",
    "    for param_name, output_size in parameter_output_mapping.items()\n",
    "})\n",
    "\n",
    "# Complete model with one Pre-trained Encoder and multiple Decoders\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoders):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = decoders\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        print(x.size())\n",
    "        batch_size = x.size(0)  # Get the batch size\n",
    "        x = x.view(batch_size, -1)  # Flatten the feature tensor, considering the batch size\n",
    "        decoder_outputs = {param_name: decoder(x) for param_name, decoder in self.decoders.items()}\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "# Create an instance of the EncoderDecoderModel with a pre-trained encoder\n",
    "encoder = Encoder()  # Use your pre-trained encoder here\n",
    "model = EncoderDecoderModel(encoder, decoders)\n",
    "\n",
    "# Example input tensor with shape (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(2, 1, 512, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output shapes for each decoder\n",
    "for param_name, decoder_output in output.items():\n",
    "    print(f\"{param_name} decoder output shape: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoders (own) working from colab\n",
    "\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Encoder using VGG (you may replace it with your desired encoder)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # size: 512x512\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # size: 256x256\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # size: 256x256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # size: 128x128\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # size: 128x128 \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # size: 64x64\n",
    "            nn.Flatten(),  # size: 64x64\n",
    "            nn.Linear(64 * 64 * 64, 4096)  # size: 4096\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Define Decoder using a simple 3-layer MLP\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the ranges from the YAML file\n",
    "with open('./ranges.yml', 'r') as file:\n",
    "    ranges = yaml.safe_load(file)\n",
    "\n",
    "# Create a mapping between parameter names and output sizes\n",
    "parameter_output_mapping = {}\n",
    "for param_name, param_specs in ranges.items():\n",
    "    if param_specs['type'] == 'float':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'int':\n",
    "        parameter_output_mapping[param_name] = 1  # 1 for scalar\n",
    "    elif param_specs['type'] == 'vector':\n",
    "        parameter_output_mapping[param_name] = 3  # 3 for x, y, z\n",
    "    elif param_specs['type'] == 'states':\n",
    "        parameter_output_mapping[param_name] = len(param_specs['values'])\n",
    "    elif param_specs['type'] == 'bool':\n",
    "        parameter_output_mapping[param_name] = 2  # 2 for binary encoding\n",
    "\n",
    "# Create decoders based on the mapping\n",
    "decoders = nn.ModuleDict({\n",
    "    param_name: Decoder(4096, output_size)\n",
    "    for param_name, output_size in parameter_output_mapping.items()\n",
    "})\n",
    "\n",
    "# Complete model with one Pre-trained Encoder and multiple Decoders\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoders):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = decoders\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        print(x.size())\n",
    "        batch_size = x.size(0)  # Get the batch size\n",
    "        x = x.view(batch_size, -1)  # Flatten the feature tensor, considering the batch size\n",
    "        decoder_outputs = {param_name: decoder(x) for param_name, decoder in self.decoders.items()}\n",
    "        return decoder_outputs\n",
    "\n",
    "# Example usage\n",
    "# Create an instance of the EncoderDecoderModel with a pre-trained encoder\n",
    "encoder = Encoder()  # Use your pre-trained encoder here\n",
    "model = EncoderDecoderModel(encoder, decoders)\n",
    "\n",
    "# Example input tensor with shape (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(2, 1, 512, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output shapes for each decoder\n",
    "for param_name, decoder_output in output.items():\n",
    "    print(f\"{param_name} decoder output shape: {decoder_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_name: str, datasets_folder: str=\"./datasets\", transform=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.datasets_folder = datasets_folder\n",
    "        self.dataset_path = os.path.join(self.datasets_folder, self.dataset_name)\n",
    "        self.images_folder = os.path.join(self.dataset_path, \"images\")\n",
    "        self.params_folder = os.path.join(self.dataset_path, \"params\")\n",
    "        self.ranges_file_path = os.path.join(self.dataset_path, \"ranges.yml\")\n",
    "        self.ranges = None\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.Resize((512, 512)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "            ) if transform is None else transform\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the ranges from the YAML file\n",
    "        with open(self.ranges_file_path, 'r') as file:\n",
    "            self.ranges = yaml.safe_load(file)\n",
    "        # read images and parameters\n",
    "        data = []\n",
    "        for image_name in os.listdir(self.images_folder):\n",
    "            image_path = os.path.join(self.images_folder, image_name)\n",
    "            param_path = os.path.join(self.params_folder, os.path.splitext(image_name)[0] + \".yml\")\n",
    "            with open(param_path, 'r') as file:\n",
    "                param = yaml.safe_load(file)\n",
    "            # normalize\n",
    "            param = self.preprocess(param)\n",
    "            data.append((image_path, param))\n",
    "        return data\n",
    "\n",
    "    def preprocess(self, param):\n",
    "        processed_param = {}\n",
    "        # for float and vector: normalize with min max\n",
    "        # for states, bool: convert to one hot\n",
    "        # for ints: treat as float, but round back to int when saving as param\n",
    "        for param_name, param_spec in self.ranges.items():\n",
    "            if param_spec['type'] == 'float' or param_spec['type'] == 'int' or param_spec['type'] == 'vector':\n",
    "                processed_param[param_name] = self.normalize(param[param_name], param_spec)\n",
    "            elif param_spec['type'] == 'states' or param_spec['type'] == 'bool':\n",
    "                processed_param[param_name] = self.one_hot(param[param_name], param_spec)\n",
    "            else: \n",
    "                raise ValueError(f\"Unsupported parameter type: {param_spec['type']}\")\n",
    "        return processed_param\n",
    "    \n",
    "    def normalize(self, value, param_spec):\n",
    "        if param_spec['type'] == 'float' or param_spec['type'] == 'int':\n",
    "            return (value - param_spec['min']) / (param_spec['max'] - param_spec['min'])\n",
    "        elif param_spec['type'] == 'vector':\n",
    "            return [(value[i] - param_spec[f'{dim}min']) / (param_spec[f'{dim}max'] - param_spec[f'{dim}min']) for i, dim in enumerate(['x', 'y', 'z'])]\n",
    "        else: \n",
    "            raise ValueError(f\"Unsupported parameter type: {param_spec['type']}\")\n",
    "\n",
    "    def one_hot(self, value, param_spec):\n",
    "        if param_spec['type'] == 'states':\n",
    "            index = param_spec['values'].index(value)\n",
    "            return [1 if i == index else 0 for i in range(len(param_spec['values']))]\n",
    "        elif param_spec['type'] == 'bool':\n",
    "            # make bools onehot too to make it consistent\n",
    "            return [1, 0] if value else [0, 1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter type: {param_spec['type']}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, target = self.data[idx]\n",
    "        sample = Image.open(sample).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # convert target's values to tensor if it's not already\n",
    "        for key, value in target.items():\n",
    "            if not isinstance(value, torch.Tensor):\n",
    "                target[key] = torch.tensor(value, dtype=torch.float32)\n",
    "\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DAGDataset\n",
    "dataset = DAGDataset(\"test_dataset\")\n",
    "\n",
    "# split into train val and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model\n",
    "encoder = Encoder()\n",
    "decoders = nn.ModuleDict({\n",
    "    param_name: Decoder(4096, output_size)\n",
    "    for param_name, output_size in parameter_output_mapping.items()\n",
    "})\n",
    "model = EncoderDecoderModel(encoder, decoders)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# for regression, use MSELoss, for classification, use CrossEntropyLoss\n",
    "class EncDecsLoss(nn.Module):\n",
    "    def __init__(self, decoders):\n",
    "        super(EncDecsLoss, self).__init__()\n",
    "        self.decoders = decoders\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = 0.0\n",
    "        for param_name, decoder_output in outputs.items():\n",
    "            decoder = self.decoders[param_name]\n",
    "            loss += decoder_loss(decoder_output, targets[param_name])\n",
    "        return loss\n",
    "\n",
    "def decoder_loss(decoder_output, target):\n",
    "    # Define your decoder-specific loss function here\n",
    "    # For example, you can use mean squared error (MSE) loss for regression\n",
    "    # or cross-entropy loss for classification\n",
    "    if decoder_output.size(-1) == 1:\n",
    "        # get rid of unnecessary dimension\n",
    "        decoder_output = decoder_output.squeeze(-1)\n",
    "        return nn.MSELoss()(decoder_output, target)\n",
    "    else:\n",
    "        return nn.CrossEntropyLoss()(decoder_output, target)\n",
    "\n",
    "criterion = EncDecsLoss(decoders)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with train and val\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, targets = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, targets = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "        print(f\"Validation loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Save your trained model if needed\n",
    "torch.save(model.state_dict(), \"encDecModel.pth\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, targets = data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "    print(f\"Test loss: {test_loss / len(test_loader)}\")\n",
    "print(\"Finished Testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
